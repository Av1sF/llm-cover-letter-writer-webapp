{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Quantified Phi 3.5 Mini to HF Cover Letter Dataset\n",
    "Fine-tuning Phi 3.5 Mini using QLoRa with MLflow and PEFT\n",
    "1. Using QLoRa and PEFT to overcome the GPU limitations for fine-tuning on [`microsoft/Phi-3.5-mini-instruct` ](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)\n",
    "2. Utilise MLflow to log model artifacts, hyperparameters, metrics, and prompts \n",
    "3. Saving the fine-tuned model and potentially testing it.\n",
    "\n",
    "Brief overview of the technologies used: \n",
    "* [QLoRA](https://github.com/artidoro/qlora) allows us to fine-tune large foundational models with limited GPU resources. It reduces the number of trainable parameters and also applies 4-bit quantization to the frozen pretrained model to further reduce the memory footprint.\n",
    "* [PEFT](https://huggingface.co/docs/peft/en/index) with PEFT, you can apply QLoRA to the pretrained model with a few lines of configurations like the normal Transformers model training.\n",
    "\n",
    "Reference/useful notebooks: \n",
    "1) https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-peft.html\n",
    "2) https://discuss.huggingface.co/t/tutorial-phi-3-5-fine-tuning/103461 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that pytorch is working with CUDA to utilise GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "['sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'sm_37', 'compute_37']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_arch_list())\n",
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ROG\\test-cloud-coursework\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# data \n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# loading model and training \n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          TrainingArguments, \n",
    "                          BitsAndBytesConfig)\n",
    "# mlflow \n",
    "import mlflow\n",
    "import datetime \n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset in pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit -> https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-peft.html  (Apache-2.0 license) \n",
    "\n",
    "# displays sample of dataset \n",
    "def display_table(dataset_or_sample):\n",
    "    # A helper fuction to display a Transformer dataset or single sample contains multi-line string nicely\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "    pd.set_option(\"display.width\", None)\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "    if isinstance(dataset_or_sample, dict):\n",
    "        df = pd.DataFrame(dataset_or_sample, index=[0])\n",
    "    else:\n",
    "        df = pd.DataFrame(dataset_or_sample)\n",
    "\n",
    "    html = df.to_html().replace(\"\\\\n\", \"<br>\")\n",
    "    styled_html = f\"\"\"<style> .dataframe th, .dataframe tbody td {{ text-align: left; padding-right: 30px; }} </style> {html}\"\"\"\n",
    "    display(HTML(styled_html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Preferred Qualifications</th>\n",
       "      <th>Hiring Company</th>\n",
       "      <th>Applicant Name</th>\n",
       "      <th>Past Working Experience</th>\n",
       "      <th>Current Working Experience</th>\n",
       "      <th>Skillsets</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Cover Letter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Java Developer</td>\n",
       "      <td>5+ years of experience in Java Development</td>\n",
       "      <td>Google</td>\n",
       "      <td>John Doe</td>\n",
       "      <td>Java Developer at XYZ for 3 years</td>\n",
       "      <td>Senior Java Developer at ABC for 2 years</td>\n",
       "      <td>Java, Spring Boot, Hibernate, SQL</td>\n",
       "      <td>BSc in Computer Science</td>\n",
       "      <td>I am writing to express my interest in the Senior Java Developer position at Google. With over 5 years of experience in Java development, I am confident in my ability to contribute effectively to your team. My professional experience includes designing and implementing Java applications, managing the full software development lifecycle, and troubleshooting and resolving technical issues. I also possess strong skills in Spring Boot, Hibernate and SQL. I am a diligent and dedicated professional, always looking to improve and learn new skills. I am excited about the opportunity to work with Google and contribute to your ongoing projects. I am certain that my skills and experience make me a strong candidate for this position.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"ShashiVish/cover-letter-dataset\"\n",
    "\n",
    "# only use 5 percent of the dataset \n",
    "train_dataset = load_dataset(dataset_name, split=\"train[:5%]\")\n",
    "test_dataset = load_dataset(dataset_name, split=\"test[:5%]\")\n",
    "\n",
    "display_table(train_dataset.select(range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset contains 41 cv-to-coverletter pairs\n",
      "Test dataset contains 17 cv-to-coverletter pairs\n",
      "['Job Title', 'Preferred Qualifications', 'Hiring Company', 'Applicant Name', 'Past Working Experience', 'Current Working Experience', 'Skillsets', 'Qualifications', 'Cover Letter']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset contains {len(train_dataset)} cv-to-coverletter pairs\")\n",
    "print(f\"Test dataset contains {len(test_dataset)} cv-to-coverletter pairs\")\n",
    "column_names = list(train_dataset.features)\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise and Format Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse dataset into chat templating. Based on this documentation https://huggingface.co/docs/transformers/en/chat_templating\n",
    "\n",
    "An example of the chat templating format: \n",
    ">`messages = [ ` \\\n",
    "> `    {\"role\": \"user\", \"content\": \"Hi there!\"},`  \\\n",
    "> `    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"},`\\\n",
    ">`    {\"role\": \"user\", \"content\": \"Can I ask a question?\"}`\\\n",
    ">`]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_message_template(row): \n",
    "    messages = [\n",
    "        # system prompt \n",
    "        {\"content\": \n",
    "         \"\"\"You are a powerful cover letter generator. Generate a professional formal personalised cover letter based on job title, qualifications, hiring company name, applicant name, work experience, skills. The cover letter must be between 300 - 400 words.\"\"\", \n",
    "         \"role\": \"system\"},\n",
    "         # Format database information into prompt \n",
    "        {\"content\": \n",
    "        f\"\"\"Generate Cover Letter using this information:\n",
    "        Job Title: {row['Job Title']}, Preferred Qualifications: {row['Preferred Qualifications']}, Hiring Company: {row['Hiring Company']}, Applicant Name: {row['Applicant Name']}, Past Working Experience: {row['Past Working Experience']}, Current Working Experience: {row['Current Working Experience']}, Skillsets:{row['Skillsets']}, Qualifications: {row['Qualifications']}\"\"\",\n",
    "        \"role\" : \"user\"},\n",
    "        # ideal response from assistant \n",
    "        {\"content\": f\"{row['Cover Letter']}\", \"role\":\"assistant\"}\n",
    "    ] \n",
    "    return {\"messages\":messages} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'You are a powerful cover letter generator. Generate a professional formal personalised cover letter based on job title, qualifications, hiring company name, applicant name, work experience, skills. The cover letter must be between 300 - 400 words.', 'role': 'system'}, {'content': 'Generate Cover Letter using this information:\n",
       "        Job Title: Senior Java Developer, Preferred Qualifications: 5+ years of experience in Java Development, Hiring Company: Google, Applicant Name: John Doe, Past Working Experience: Java Developer at XYZ for 3 years, Current Working Experience: Senior Java Developer at ABC for 2 years, Skillsets:Java, Spring Boot, Hibernate, SQL, Qualifications: BSc in Computer Science', 'role': 'user'}, {'content': 'I am writing to express my interest in the Senior Java Developer position at Google. With over 5 years of experience in Java development, I am confident in my ability to contribute effectively to your team. My professional experience includes designing and implementing Java applications, managing the full software development lifecycle, and troubleshooting and resolving technical issues. I also possess strong skills in Spring Boot, Hibernate and SQL. I am a diligent and dedicated professional, always looking to improve and learn new skills. I am excited about the opportunity to work with Google and contribute to your ongoing projects. I am certain that my skills and experience make me a strong candidate for this position.', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transform dataset to chat templating \n",
    "train_dataset = train_dataset.map(apply_message_template,\n",
    "                                  remove_columns=column_names)\n",
    "\n",
    "# display our transformed dataset \n",
    "display_table(train_dataset.select(range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'You are a powerful cover letter generator. Generate a professional formal personalised cover letter based on job title, qualifications, hiring company name, applicant name, work experience, skills. The cover letter must be between 300 - 400 words.', 'role': 'system'}, {'content': 'Generate Cover Letter using this information:\n",
       "        Job Title:  Data Scientist, Preferred Qualifications: BSc focused on data Science/computer Science/engineering\n",
       "4+ years experience Developing and shipping production grade machine learning systems\n",
       "2+ years building and shipping data Science based personalization services and recommendation systems\n",
       "experience in data Science or machine learning engineering\n",
       "Strong analytical and data Science skills, Hiring Company: XYZ Corporation, Applicant Name: John Smith, Past Working Experience: Data Analyst at ABC Company, Current Working Experience: Machine Learning Engineer at DEF Company, Skillsets:Python, R, scikit-learn, Keras, Tensorflow, Qualifications: BSc in Computer Science, 5+ years of experience in data science and machine learning', 'role': 'user'}, {'content': 'Dear Hiring Manager,\n",
       "\n",
       "I am writing to express my interest in the Data Scientist position at XYZ Corporation. With my strong background in data science and machine learning, I believe I am well-suited for this role.\n",
       "\n",
       "In my previous role as a Data Analyst at ABC Company, I gained experience in identifying and engineering features for modeling. I also have a proven track record of evaluating various modeling techniques and developing models. Additionally, my current position as a Machine Learning Engineer at DEF Company has allowed me to collaborate with stakeholders and put models into production.\n",
       "\n",
       "I have a BSc in Computer Science and over 5 years of experience in data science and machine learning. I am proficient in Python, R, scikit-learn, Keras, and Tensorflow. I am eager to learn from others and contribute to the growth of the team.\n",
       "\n",
       "I am confident that my strong analytical and data science skills, along with my ability to work well in cross-functional teams, make me a valuable asset to XYZ Corporation. I am excited about the opportunity to contribute to the development of personalization services and recommendation systems.\n",
       "\n",
       "Thank you for considering my application. I look forward to the opportunity to discuss how my skills and qualifications align with the needs of XYZ Corporation.\n",
       "\n",
       "Sincerely,\n",
       "John Smith', 'role': 'assistant'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = test_dataset.map(apply_message_template,\n",
    "                                  remove_columns=column_names)\n",
    "\n",
    "display_table(test_dataset.select(range(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and set up MLFlow tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalise MLFlow for tracking parameters and setting up our run and experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"QUANT PHI PEFT\")\n",
    "run_name = f\"Phi-3.5-mini-Cover-Letter-QLoRA-{(datetime.datetime.now()).strftime('%Y-%m-%d-%H:%M:%S')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs \n",
    "\n",
    "training_config = {\n",
    "    \"report_to\": \"mlflow\",\n",
    "    \"run_name\": run_name,\n",
    "    \"fp16\":True,\n",
    "    \"bf16\": False,\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 5.0e-06,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 20,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./checkpoint_dir\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    }\n",
    "\n",
    "peft_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    \"modules_to_save\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our configurations \n",
    "train_conf = TrainingArguments(**training_config)\n",
    "peft_conf = LoraConfig(**peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "# Load the model with 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    # Use double quantization\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    # Use 4-bit Normal Float for storing the base model weights in GPU memory\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # De-quantize the weights to 16-bit (Brain) float before the forward/backward pass\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# pass quantisation configuration to our model \n",
    "model_kwargs = dict(\n",
    "    quantization_config=quantization_config, \n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ")\n",
    "\n",
    "# initalise model and tokeniser \n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 2048\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data using our tokenizer \n",
    "# Data has to be processed twice as tokenizer process prompts well if it is not in hugging face's chat template \n",
    "def apply_chat_template(\n",
    "    record,\n",
    "    tokenizer,\n",
    "):\n",
    "    messages = record[\"messages\"]\n",
    "    record[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False)\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(train_dataset.features)\n",
    "\n",
    "processed_train_dataset = train_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    # remove irrelevant columns\n",
    "    remove_columns=column_names,\n",
    "    num_proc=10,\n",
    "    desc=\"Applying chat template to train_sft\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_dataset = test_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    # remove irrelevant columns\n",
    "    remove_columns=column_names,\n",
    "    num_proc=10,\n",
    "    desc=\"Applying chat template to test_sft\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;<br>You are a powerful cover letter generator. Generate a professional formal personalised cover letter based on job title, qualifications, hiring company name, applicant name, work experience, skills. The cover letter must be between 300 - 400 words.&lt;|end|&gt;<br>&lt;|user|&gt;<br>Generate Cover Letter using this information:<br>        Job Title: Senior Java Developer, Preferred Qualifications: 5+ years of experience in Java Development, Hiring Company: Google, Applicant Name: John Doe, Past Working Experience: Java Developer at XYZ for 3 years, Current Working Experience: Senior Java Developer at ABC for 2 years, Skillsets:Java, Spring Boot, Hibernate, SQL, Qualifications: BSc in Computer Science&lt;|end|&gt;<br>&lt;|assistant|&gt;<br>I am writing to express my interest in the Senior Java Developer position at Google. With over 5 years of experience in Java development, I am confident in my ability to contribute effectively to your team. My professional experience includes designing and implementing Java applications, managing the full software development lifecycle, and troubleshooting and resolving technical issues. I also possess strong skills in Spring Boot, Hibernate and SQL. I am a diligent and dedicated professional, always looking to improve and learn new skills. I am excited about the opportunity to work with Google and contribute to your ongoing projects. I am certain that my skills and experience make me a strong candidate for this position.&lt;|end|&gt;<br>&lt;|endoftext|&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;<br>You are a powerful cover letter generator. Generate a professional formal personalised cover letter based on job title, qualifications, hiring company name, applicant name, work experience, skills. The cover letter must be between 300 - 400 words.&lt;|end|&gt;<br>&lt;|user|&gt;<br>Generate Cover Letter using this information:<br>        Job Title:  Data Scientist, Preferred Qualifications: BSc focused on data Science/computer Science/engineering<br>4+ years experience Developing and shipping production grade machine learning systems<br>2+ years building and shipping data Science based personalization services and recommendation systems<br>experience in data Science or machine learning engineering<br>Strong analytical and data Science skills, Hiring Company: XYZ Corporation, Applicant Name: John Smith, Past Working Experience: Data Analyst at ABC Company, Current Working Experience: Machine Learning Engineer at DEF Company, Skillsets:Python, R, scikit-learn, Keras, Tensorflow, Qualifications: BSc in Computer Science, 5+ years of experience in data science and machine learning&lt;|end|&gt;<br>&lt;|assistant|&gt;<br>Dear Hiring Manager,<br><br>I am writing to express my interest in the Data Scientist position at XYZ Corporation. With my strong background in data science and machine learning, I believe I am well-suited for this role.<br><br>In my previous role as a Data Analyst at ABC Company, I gained experience in identifying and engineering features for modeling. I also have a proven track record of evaluating various modeling techniques and developing models. Additionally, my current position as a Machine Learning Engineer at DEF Company has allowed me to collaborate with stakeholders and put models into production.<br><br>I have a BSc in Computer Science and over 5 years of experience in data science and machine learning. I am proficient in Python, R, scikit-learn, Keras, and Tensorflow. I am eager to learn from others and contribute to the growth of the team.<br><br>I am confident that my strong analytical and data science skills, along with my ability to work well in cross-functional teams, make me a valuable asset to XYZ Corporation. I am excited about the opportunity to contribute to the development of personalization services and recommendation systems.<br><br>Thank you for considering my application. I look forward to the opportunity to discuss how my skills and qualifications align with the needs of XYZ Corporation.<br><br>Sincerely,<br>John Smith&lt;|end|&gt;<br>&lt;|endoftext|&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display samples from our process datasets \n",
    "display_table(processed_train_dataset.select(range(1)))\n",
    "display_table(processed_test_dataset.select(range(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ROG\\test-cloud-coursework\\.conda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a powerful cover letter generator. Generate a professional formal personalised cover letter based on job title, qualifications, hiring company name, applicant name, work experience, skills. The cover letter must be between 300 - 400 words.', 'role': 'system'}, {'content': 'Generate Cover Letter using this information:\\n        Job Title: Senior Java Developer, Preferred Qualifications: 5+ years of experience in Java Development, Hiring Company: Google, Applicant Name: John Doe, Past Working Experience: Java Developer at XYZ for 3 years, Current Working Experience: Senior Java Developer at ABC for 2 years, Skillsets:Java, Spring Boot, Hibernate, SQL, Qualifications: BSc in Computer Science', 'role': 'user'}]\n",
      " [John Doe]\n",
      "[John's Address]\n",
      "[City, State, Zip]\n",
      "[Email Address]\n",
      "[Phone Number]\n",
      "[Date]\n",
      "\n",
      "Hiring Manager\n",
      "Google Inc.\n",
      "[Google's Address]\n",
      "[City, State, Zip]\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I am writing to express my interest in the Senior Java Developer position at Google as advertised. With a Bachelor of Science in Computer Science and over six years of hands-on experience in Java development, I am confident in my ability to contribute to your team and help drive innovation at Google.\n",
      "\n",
      "During my tenure as a Java Developer at XYZ, I honed my skills in Java, Spring Boot, Hibernate, and SQL. I successfully led a team of developers in the development of a robust e-commerce platform that increased the company's revenue by 20% within the first year of its launch. My experience at XYZ has equipped me with a deep understanding of the Java ecosystem and the ability to develop high-quality, scalable applications that meet the needs of end-users.\n",
      "\n",
      "In my current role as a Senior Java Developer at ABC, I have had the opportunity to work on several high-profile projects. I have been responsible for designing, developing, and maintaining complex Java applications that have been instrumental in driving the company's growth. I have also had the chance to work closely with cross-functional teams, including product managers, UX designers, and QA engineers, to ensure that the final product meets the needs of the end-users and aligns with the company's strategic goals.\n",
      "\n",
      "My experience has also given me a strong understanding of the latest trends and best practices in the Java ecosystem. I am well-versed in the latest Java frameworks, libraries, and tools, and I am always eager to learn new technologies and techniques to stay ahead of the curve. I am particularly impressed by Google's commitment to innovation and its focus on developing cutting-edge technologies that have the potential to transform the industry.\n",
      "\n",
      "I am excited about the opportunity to join Google and contribute to your team's success. I am confident that my experience, skills, and passion for Java development will enable me to make a significant impact at Google. I am particularly interested in working on projects that involve developing scalable, high-performance Java applications that can be used by millions of users worldwide.\n",
      "\n",
      "Thank you for considering my application. I would be delighted to discuss my qualifications further and learn more about the Senior Java Developer position at Google. I am available for an interview at your earliest convenience and can be reached at [Phone Number] or via email at [Email Address].\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "John Doe\n",
      "\n",
      "[Enclosures: Resume]\n"
     ]
    }
   ],
   "source": [
    "# initalise pipeline with quanitised model \n",
    "pipeline = transformers.pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\")\n",
    "\n",
    "# following configurations specified at Phi 3.5 model card \n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 700,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "messages = train_dataset[0][\"messages\"][0:2]\n",
    "print(messages)\n",
    "\n",
    "# generate without assistant prompt\n",
    "with torch.no_grad():\n",
    "    output = pipeline(messages, **generation_args)\n",
    "\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "#generated output in 1m 25.9s on GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a powerful cover letter generator. Generate a professional formal personalised cover letter based on job title, qualifications, hiring company name, applicant name, work experience, skills. The cover letter must be between 300 - 400 words.', 'role': 'system'}, {'content': 'Generate Cover Letter using this information:\\n        Job Title: Senior Java Developer, Preferred Qualifications: 5+ years of experience in Java Development, Hiring Company: Google, Applicant Name: John Doe, Past Working Experience: Java Developer at XYZ for 3 years, Current Working Experience: Senior Java Developer at ABC for 2 years, Skillsets:Java, Spring Boot, Hibernate, SQL, Qualifications: BSc in Computer Science', 'role': 'user'}, {'content': 'I am writing to express my interest in the Senior Java Developer position at Google. With over 5 years of experience in Java development, I am confident in my ability to contribute effectively to your team. My professional experience includes designing and implementing Java applications, managing the full software development lifecycle, and troubleshooting and resolving technical issues. I also possess strong skills in Spring Boot, Hibernate and SQL. I am a diligent and dedicated professional, always looking to improve and learn new skills. I am excited about the opportunity to work with Google and contribute to your ongoing projects. I am certain that my skills and experience make me a strong candidate for this position.', 'role': 'assistant'}]\n",
      "\n",
      "\n",
      "My name is John Doe, and I hold a Bachelor of Science degree in Computer Science. I have worked as a Java Developer at XYZ for three years, where I gained valuable experience in developing and maintaining Java applications. During my tenure at XYZ, I was responsible for designing and implementing Java applications that met the company's requirements. I also worked closely with cross-functional teams to ensure that the applications were delivered on time and within budget.\n",
      "\n",
      "In my current role as a Senior Java Developer at ABC, I have had the opportunity to work on more complex projects. I have been responsible for designing and implementing large-scale Java applications that have been used by thousands of users. I have also been involved in the full software development lifecycle, from requirements gathering to testing and deployment. I have used Spring Boot, Hibernate, and SQL to build robust and scalable applications that meet the needs of our users.\n",
      "\n",
      "In addition to my technical skills, I am a strong team player and have experience working in a fast-paced environment. I am a proactive problem-solver and have a strong attention to detail. I am also skilled in Agile methodologies and have experience working in a Scrum team. I am confident that my experience and skills make me a strong candidate for the Senior Java Developer position at Google.\n",
      "\n",
      "I am excited about the opportunity to work with Google and contribute to your ongoing projects. I am confident that my experience and skills will enable me to make a significant contribution to your team. I am eager to learn more about the specific projects you are working on and how I can help. I am available for an interview at your earliest convenience and can be reached at john.doe@email.com or (123) 456-7890.\n",
      "\n",
      "Thank you for considering my application. I look forward to the opportunity to discuss my qualifications with you further.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "John Doe\n",
      "\n",
      "(123) 456-7890\n",
      "\n",
      "john.doe@email.com\n"
     ]
    }
   ],
   "source": [
    "# testing model with assistant message \n",
    "messages = train_dataset[0][\"messages\"]\n",
    "print(messages)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = pipeline(messages, **generation_args)\n",
    "\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "# generated in 1m 1.9s on GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune quantified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ROG\\test-cloud-coursework\\.conda\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field, packing. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\ROG\\test-cloud-coursework\\.conda\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:212: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ROG\\test-cloud-coursework\\.conda\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ROG\\test-cloud-coursework\\.conda\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# initalise model trainer with our peft conf \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_conf,\n",
    "    peft_config=peft_conf,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_test_dataset,\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 7\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7\n",
      "  Number of trainable parameters = 562,176\n",
      "100%|██████████| 7/7 [00:15<00:00,  1.88s/it]Saving model checkpoint to ./checkpoint_dir\\checkpoint-7\n",
      "c:\\Users\\ROG\\test-cloud-coursework\\.conda\\Lib\\site-packages\\peft\\utils\\save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./checkpoint_dir\\checkpoint-7\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint_dir\\checkpoint-7\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 7/7 [00:16<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16.8211, 'train_samples_per_second': 0.416, 'train_steps_per_second': 0.416, 'train_loss': 1.9283688408987862, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =   298255GF\n",
      "  train_loss               =     1.9284\n",
      "  train_runtime            = 0:00:16.82\n",
      "  train_samples_per_second =      0.416\n",
      "  train_steps_per_second   =      0.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/11 01:50:33 INFO mlflow.tracking._tracking_service.client: 🏃 View run capable-quail-412 at: http://127.0.0.1:5000/#/experiments/352351548751262593/runs/89af066455ba479883e38e18391ab4cc.\n",
      "2024/11/11 01:50:33 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/352351548751262593.\n"
     ]
    }
   ],
   "source": [
    "# train our model \n",
    "with mlflow.start_run() as run:\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models\n",
      "c:\\Users\\ROG\\test-cloud-coursework\\.conda\\Lib\\site-packages\\peft\\utils\\save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./saved_models\\tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# save trained model \n",
    "trainer.save_model(\"./saved_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the fine-tuned quant model \n",
    "Logging the model in MLFLOW. \n",
    "\n",
    "First we register a signature to ensure MLflow knows what type of inputs the model can take. Unluckily, we cannot pass the HF chat template dict type as MLFlow only accept signatures of string type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [string (required)]\n",
       "outputs: \n",
       "  [string (required)]\n",
       "params: \n",
       "  ['max_new_tokens': integer (default: 700), 'return_full_text': boolean (default: False), 'temperature': double (default: 0.0), 'do_sample': boolean (default: False)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve a sample input from our dataset \n",
    "sample = processed_test_dataset['text']\n",
    "\n",
    "# register a signature \n",
    "signature = infer_signature(\n",
    "    model_input=sample,\n",
    "    model_output=sample,\n",
    "    # save the generation configs we used \n",
    "    params=generation_args,\n",
    ")\n",
    "\n",
    "# display signature \n",
    "signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at C:\\Users\\ROG\\.cache\\huggingface\\hub\\models--microsoft--Phi-3.5-mini-instruct\\snapshots\\af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0\\tokenizer.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\ROG\\.cache\\huggingface\\hub\\models--microsoft--Phi-3.5-mini-instruct\\snapshots\\af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0\\tokenizer.json\n",
      "loading file added_tokens.json from cache at C:\\Users\\ROG\\.cache\\huggingface\\hub\\models--microsoft--Phi-3.5-mini-instruct\\snapshots\\af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\ROG\\.cache\\huggingface\\hub\\models--microsoft--Phi-3.5-mini-instruct\\snapshots\\af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ROG\\.cache\\huggingface\\hub\\models--microsoft--Phi-3.5-mini-instruct\\snapshots\\af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0\\tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024/11/11 01:13:46 INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\n",
      "c:\\Users\\ROG\\test-cloud-coursework\\.conda\\Lib\\site-packages\\peft\\utils\\save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "2024/11/11 01:13:47 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained argumentis set to False. The reference to the HuggingFace Hub repository microsoft/Phi-3.5-mini-instruct will be logged instead.\n",
      "2024/11/11 01:13:48 INFO mlflow.transformers: text-generation pipelines saved with prompt templates have the `return_full_text` pipeline kwarg set to False by default. To override this behavior, provide a `model_config` dict with `return_full_text` set to `True` when saving the model.\n",
      "2024/11/11 01:13:48 INFO mlflow.transformers: A local checkpoint path or PEFT model is given as the `transformers_model`. To avoid loading the full model into memory, we don't infer the pip requirement for the model. Instead, we will use the default requirements, but it may not capture all required pip libraries for the model. Consider providing the pip requirements explicitly.\n",
      "2024/11/11 01:13:55 INFO mlflow.tracking._tracking_service.client: 🏃 View run incongruous-crab-806 at: http://127.0.0.1:5000/#/experiments/352351548751262593/runs/bd28f70e10a14a1bb11e3d9fcf5f0238.\n",
      "2024/11/11 01:13:55 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/352351548751262593.\n"
     ]
    }
   ],
   "source": [
    "# retrieve run ID of our fine-tuned model \n",
    "last_run_id = mlflow.last_active_run().info.run_id\n",
    "\n",
    "# Save tokenizer without padding because it is only needed for training\n",
    "tokenizer_no_pad = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "prompt_template = \"\"\"You are a powerful cover letter generator. Generate a professional formal personalised cover letter based on job title, qualifications, hiring company name, applicant name, work experience, skills. The cover letter must be between 300 - 400 words.\n",
    "        {prompt}\n",
    "         \"\"\"\n",
    "\n",
    "# register the model with MLFlow \n",
    "with mlflow.start_run(run_id=last_run_id):\n",
    "    mlflow.log_params(peft_config)\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer_no_pad},\n",
    "        prompt_template=prompt_template,\n",
    "        signature=signature,\n",
    "        artifact_path=\"model\",  \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to run our finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model in based on Run ID found from MLflow UI \n",
    "mlflow_model = mlflow.pyfunc.load_model(\"runs:/62da25f1c43c4623815db5e2d4e26cb8/model\")\n",
    "\n",
    "# We only input table and question, since system prompt is adeed in the prompt template.\n",
    "test_dataset = load_dataset(dataset_name, split=\"test[:5%]\")\n",
    "sample = test_dataset[1]\n",
    "\n",
    "# MLflow infers schema from the provided sample input/output/params\n",
    "model_input={\n",
    "    \"Job Title\": sample['Job Title'],\n",
    "    \"Work Experience\": f\"{sample['Current Working Experience']},{sample['Past Working Experience']}\",\n",
    "    \"Preferred Qualifications\" : sample['Preferred Qualifications'], \n",
    "    \"Qualitifcations\" : sample['Qualifications'],\n",
    "    \"Hiring Company\" : sample['Hiring Company'],\n",
    "    \"Applicant Name\" : sample['Applicant Name'],\n",
    "    \"Skillsets\" : sample['Skillsets'],        \n",
    "}\n",
    "# Inference parameters like max_tokens_length are set to default values specified in the Model Signature\n",
    "generated_query = mlflow_model.predict(model_input)[0]\n",
    "display_table({\"prompt\": model_input, \"generated_query\": generated_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps \n",
    "Attempted to test the model. The model loaded in successfully, however inference took over more than an hour and eventually timed-out. I read about other users experiencing the same problem with slower inference after fine-tuning their model with PEFT. I tested it with just fine-tuning a small part of the model using PEFT but it was unsuccessful due to my hardware GPU limitations. \n",
    "\n",
    "Furthermore, I later discovered that this would not be able to be implemented a quantified model in the final product, due to how quantification works in Hugging face. Hugging face requires that the full model to be downloaded, then the model has to be quantified utilising libraries such as, BitsAndBytes. These require the use of a GPU which is not an allow specification in the coursework outline. The unsuitability of this model is further highlighted through it's memory foot print which reached up to 10 GB. \n",
    "\n",
    "Hence, to combat this and attempt to speed up inference time. I will test other model pipelines exclusively on CPU first to ensure that the inference time is acceptable. I could limit my search to models with parameters under 1B, however this could have a heavy impact on the quality of the model output. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
